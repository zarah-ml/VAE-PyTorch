{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convolutional_Variational_Autoencoder.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNG2+ATTtAi8nxkZogG6hvo"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fBZWgn2PLBs"
      },
      "source": [
        "# **Using Variational Autoencoder to Generate Images**\n",
        "\n",
        "Variational Autoencoders (VAE) are powerful generative models having diverse applications ranging from generating fake faces to cool synthetic music. In this repo a VAE is trained on Fashion MNIST dataset to generate fake clothing images. We start off by importing required libraries and defining the loss function. The loss function includes the Binary Cross Entropy (BCE) loss used to measure reconstruction loss as well as Kullbackâ€“Leibler divergence (KL divergence) loss. KL divergence allows the generated encodings, to be as close as possible to each other while still being distinct, allowing smooth interpolation, and enabling the construction of new samples.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mlmP9hWCcdW"
      },
      "source": [
        "!pip3 install torch==1.4.0+cu100 torchvision==0.5.0+cu100 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZIZ4iW13Qw2"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import make_grid, save_image\n",
        "\n",
        "import imageio\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "matplotlib.style.use('ggplot')\n",
        "\n",
        "to_pil_image = transforms.ToPILImage()\n",
        "\n",
        "def image_to_vid(images):\n",
        "    imgs = [np.array(to_pil_image(img)) for img in images]\n",
        "    imageio.mimsave('./generated_images.gif', imgs)\n",
        "\n",
        "def save_reconstructed_images(recon_images, epoch):\n",
        "    save_image(recon_images.cpu(), f\"./output{epoch}.jpg\")\n",
        "\n",
        "def save_loss_plot(train_loss, valid_loss):\n",
        "    # loss plots\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    plt.plot(train_loss, color='orange', label='train loss')\n",
        "    plt.plot(valid_loss, color='red', label='validataion loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig('./loss.jpg')\n",
        "    plt.show()\n",
        "\n",
        "def final_loss(bce_loss, mu, logvar):\n",
        "    \"\"\"\n",
        "    This function will add the reconstruction loss (BCELoss) and the \n",
        "    KL-Divergence.\n",
        "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "    :param bce_loss: recontruction loss\n",
        "    :param mu: the mean from the latent vector\n",
        "    :param logvar: log variance from the latent vector\n",
        "    \"\"\"\n",
        "    BCE = bce_loss \n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gOvG3PTRiF4"
      },
      "source": [
        "## **Creating a Variational Autoencoder class**\n",
        "\n",
        " The required components for the class:\n",
        "\n",
        "    1. Encoder and Decoder components for VAE class.  \n",
        "    2. Parameters of the distribution and their activations in the last layer.\n",
        "    3. The sampling function, which provides the sample from the distribution with given parameters\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJ2vWGsf01q7"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,in_channels, hidden_channels, out_channels,  latent_dim, kernel_size):\n",
        "      super(Encoder,self).__init__()\n",
        "      self.enc1 = nn.Conv2d(\n",
        "              in_channels=in_channels, out_channels=hidden_channels, kernel_size=kernel_size, \n",
        "              stride=2, padding=1\n",
        "          )\n",
        "      self.enc2 = nn.Conv2d(\n",
        "              in_channels=hidden_channels, out_channels=hidden_channels*2, kernel_size=kernel_size, \n",
        "              stride=2, padding=1\n",
        "          )\n",
        "      self.enc3 = nn.Conv2d(\n",
        "              in_channels=hidden_channels*2, out_channels=hidden_channels*4, kernel_size=kernel_size, \n",
        "              stride=2, padding=1\n",
        "          )\n",
        "      self.enc4 = nn.Conv2d(\n",
        "              in_channels=hidden_channels*4, out_channels=out_channels, kernel_size=kernel_size, \n",
        "              stride=1, padding=1\n",
        "          )\n",
        "      # fully connected layers for learning representations\n",
        "      self.fc1 = nn.Linear(out_channels, 128)\n",
        "      self.fc_mu = nn.Linear(128, latent_dim)\n",
        "      self.fc_log_var = nn.Linear(128, latent_dim)\n",
        "      self.fc2 = nn.Linear(latent_dim, out_channels)\n",
        "\n",
        "  def forward(self,x):\n",
        "      x = F.relu(self.enc1(x))\n",
        "      x = F.relu(self.enc2(x))\n",
        "      x = F.relu(self.enc3(x))\n",
        "      x = F.relu(self.enc4(x))\n",
        "      batch, _, _, _ = x.shape\n",
        "      x = F.adaptive_avg_pool2d(x, 1).reshape(batch, -1)\n",
        "      hidden = self.fc1(x)\n",
        "      # get `mu` and `log_var`\n",
        "      mu = self.fc_mu(hidden)\n",
        "      log_var = self.fc_log_var(hidden)\n",
        "      # get the latent vector through reparameterization\n",
        "      z = self.reparameterize(mu, log_var)\n",
        "      z = self.fc2(z)\n",
        "      z = z.view(-1, 64, 1, 1)\n",
        "\n",
        "      return mu, log_var, z\n",
        "\n",
        "  def reparameterize(self, mu, log_var):\n",
        "        \"\"\"\n",
        "        :param mu: mean from the encoder's latent space\n",
        "        :param log_var: log variance from the encoder's latent space\n",
        "        \"\"\"\n",
        "        std = torch.exp(0.5*log_var) # standard deviation\n",
        "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
        "        sample = mu + (eps * std) # sampling\n",
        "        return sample\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self,in_channels, hidden_channels, out_channels, kernel_size):\n",
        "      super(Decoder,self).__init__()\n",
        "\n",
        "      self.dec1 = nn.ConvTranspose2d(\n",
        "            in_channels=64, out_channels=hidden_channels*8, kernel_size=kernel_size, \n",
        "            stride=1, padding=0\n",
        "        )\n",
        "      self.dec2 = nn.ConvTranspose2d(\n",
        "            in_channels=hidden_channels*8, out_channels=hidden_channels*4, kernel_size=kernel_size, \n",
        "            stride=2, padding=1\n",
        "        )\n",
        "      self.dec3 = nn.ConvTranspose2d(\n",
        "            in_channels=hidden_channels*4, out_channels=hidden_channels*2, kernel_size=kernel_size, \n",
        "            stride=2, padding=1\n",
        "        )\n",
        "      self.dec4 = nn.ConvTranspose2d(\n",
        "            in_channels=hidden_channels*2, out_channels=in_channels, kernel_size=kernel_size, \n",
        "            stride=2, padding=1\n",
        "        )\n",
        "\n",
        "  def forward(self,z):\n",
        "        x = F.relu(self.dec1(z))\n",
        "        x = F.relu(self.dec2(x))\n",
        "        x = F.relu(self.dec3(x))\n",
        "        reconstruction = torch.sigmoid(self.dec4(x))\n",
        "        return reconstruction\n",
        "       \n",
        "        \n",
        "class ConvVAE(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, latent_dim, kernel_size):\n",
        "        super(ConvVAE, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        self.encoder = Encoder(self.in_channels,self.hidden_channels, self.out_channels, self.latent_dim, self.kernel_size)\n",
        "        self.decoder = Decoder(self.in_channels,self.hidden_channels, self.out_channels, self.kernel_size)\n",
        "     \n",
        " \n",
        "    def forward(self, x):\n",
        "        mu, log_var, z = self.encoder(x)\n",
        "        reconstruction = self.decoder(z)\n",
        "                       \n",
        "        return reconstruction, mu, log_var"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uM8GUbFm9SIV"
      },
      "source": [
        "## **Training and Validation**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0L56sZod4KNU"
      },
      "source": [
        "def train(model, dataloader, dataset, device, optimizer, criterion):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    counter = 0\n",
        "    for i, data in tqdm(enumerate(dataloader), total=int(len(dataset)/dataloader.batch_size)):\n",
        "        counter += 1\n",
        "        data = data[0]\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        reconstruction, mu, logvar = model(data)\n",
        "        bce_loss = criterion(reconstruction, data)\n",
        "        loss = final_loss(bce_loss, mu, logvar)\n",
        "        loss.backward()\n",
        "        running_loss += loss.item()\n",
        "        optimizer.step()\n",
        "    train_loss = running_loss / counter \n",
        "    return train_loss\n",
        "\n",
        "def validate(model, dataloader, dataset, device, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    counter = 0\n",
        "    with torch.no_grad():\n",
        "        for i, data in tqdm(enumerate(dataloader), total=int(len(dataset)/dataloader.batch_size)):\n",
        "            counter += 1\n",
        "            data= data[0]\n",
        "            data = data.to(device)\n",
        "            reconstruction, mu, logvar = model(data)\n",
        "            bce_loss = criterion(reconstruction, data)\n",
        "            loss = final_loss(bce_loss, mu, logvar)\n",
        "            running_loss += loss.item()\n",
        "        \n",
        "            # save the last batch input and output of every epoch\n",
        "            if i == int(len(dataset)/dataloader.batch_size) - 1:\n",
        "                recon_images = reconstruction\n",
        "    val_loss = running_loss / counter\n",
        "    return val_loss, recon_images"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0_zgWbhUDZO"
      },
      "source": [
        "## **MNIST Data downloading and Parameter setting**\n",
        "Fashion-MNIST dataset comprises of 60,000 small square 28Ã—28 pixel grayscale images of items of 10 types of clothing, such as shoes, t-shirts, dresses, and more. Both the train and validation set is available from torchvision."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUsmSplZ3lR1"
      },
      "source": [
        "def main(learning_rate, batch_size, epochs):\n",
        "  \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),])\n",
        "\n",
        "    trainset = torchvision.datasets.FashionMNIST(root='./', train=True, download=True, transform=transform)\n",
        "    train_loader = DataLoader( trainset, batch_size=batch_size, shuffle=True )\n",
        "\n",
        "    # validation set and validation data loader\n",
        "    testset = torchvision.datasets.FashionMNIST(root='./', train=False, download=True, transform=transform)\n",
        "    \n",
        "    test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # set the learning parameters\n",
        "    hparams = {\n",
        "        \"in_channels\": 1,\n",
        "        \"hidden_channels\": 8,\n",
        "        \"out_channels\": 64,\n",
        "        \"latent_dim\" : 16,\n",
        "        \"kernel_size\": 4,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"epochs\": epochs,\n",
        "    }\n",
        "\n",
        "     # initialize the model\n",
        "    model = ConvVAE(hparams['in_channels'], hparams['hidden_channels'], hparams['out_channels'], hparams['latent_dim'], hparams['kernel_size']).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=hparams['learning_rate'])\n",
        "    criterion = nn.BCELoss(reduction='sum')\n",
        "\n",
        "    # a list to save all the reconstructed images in PyTorch grid format\n",
        "    grid_images = []\n",
        "    train_loss = []\n",
        "    valid_loss = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1} of {epochs}\")\n",
        "        train_epoch_loss = train(model, train_loader, trainset, device, optimizer, criterion)\n",
        "        valid_epoch_loss, recon_images = validate(model, test_loader, testset, device,  criterion)\n",
        "        train_loss.append(train_epoch_loss)\n",
        "        valid_loss.append(valid_epoch_loss)\n",
        "\n",
        "        # save the reconstructed images from the validation loop\n",
        "        save_reconstructed_images(recon_images, epoch+1)\n",
        "        # convert the reconstructed images to PyTorch image grid format\n",
        "        image_grid = make_grid(recon_images.detach().cpu())\n",
        "        grid_images.append(image_grid)\n",
        "        print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
        "        print(f\"Val Loss: {valid_epoch_loss:.4f}\")\n",
        "\n",
        "    # save the reconstructions as a .gif file\n",
        "    image_to_vid(grid_images)\n",
        "    # save the loss plots to disk\n",
        "    save_loss_plot(train_loss, valid_loss)\n",
        "    print('TRAINING COMPLETE')\n",
        "\n",
        "    sample = Variable(torch.randn(64, hparams['out_channels'],1,1))\n",
        "    sample = sample.to(device)\n",
        "    sample = model.decoder(sample).cpu()\n",
        "\n",
        "    # save out as an 8x8 matrix of MNIST digits\n",
        "    # this will give you a visual idea of how well latent space can generate things\n",
        "    # that look like digits\n",
        "    save_image(sample.data.view(64, 1, 32, 32),'./reconstruction' + str(epoch) + '.png')"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "am5ZkLGoCHiV"
      },
      "source": [
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "epochs = 30\n",
        "\n",
        "\n",
        "main(learning_rate, batch_size, epochs)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}